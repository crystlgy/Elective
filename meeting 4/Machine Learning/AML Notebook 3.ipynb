{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering Algorithm\n",
    "K-Means Clustering is an unsupervised learning algorithm that is used to solve the clustering problems in machine learning or data science. \n",
    "\n",
    "K-Means Clustering groups the unlabeled dataset into different clusters. Here K defines the number of pre-defined clusters that need to be created in the process, as if K=2, there will be two clusters, and for K=3, there will be three clusters, and so on.\n",
    "\n",
    "It is an iterative algorithm that divides the unlabeled dataset into k different clusters in such a way that each dataset belongs only one group that has similar properties.\n",
    "\n",
    "It allows us to cluster the data into different groups and a convenient way to discover the categories of groups in the unlabeled dataset on its own without the need for any training.\n",
    "\n",
    "It is a centroid-based algorithm, where each cluster is associated with a centroid. The main aim of this algorithm is to minimize the sum of distances between the data point and their corresponding clusters.\n",
    "\n",
    "The algorithm takes the unlabeled dataset as input, divides the dataset into k-number of clusters, and repeats the process until it does not find the best clusters. The value of k should be predetermined in this algorithm.\n",
    "\n",
    "The k-means clustering algorithm mainly performs two tasks:\n",
    "\n",
    "- Determines the best value for K center points or centroids by an iterative process.\n",
    "- Assigns each data point to its closest k-center. Those data points which are near to the particular k-center, create a cluster.\n",
    "\n",
    "Hence each cluster has datapoints with some commonalities, and it is away from other clusters.\n",
    "\n",
    "The below diagram explains the working of the K-means Clustering Algorithm:\n",
    "\n",
    "<img src=https://static.javatpoint.com/tutorial/machine-learning/images/k-means-clustering-algorithm-in-machine-learning.png>\n",
    "\n",
    "## Python Implementation of K-means Clustering Algorithm\n",
    "\n",
    "### Step-1: Data pre-processing Step\n",
    "The first step will be the data pre-processing, as we did in our earlier topics of Regression and Classification. But for the clustering problem, it will be different from other models. Let's discuss it:\n",
    "\n",
    "Importing Libraries\n",
    "As we did in previous topics, firstly, we will import the libraries for our model, which is part of data pre-processing. The code is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as nm    \n",
    "import matplotlib.pyplot as mtp    \n",
    "import pandas as pd    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset  \n",
    "dataset = pd.read_csv('Mall_Customers.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Age</th>\n",
       "      <th>Annual Income (k$)</th>\n",
       "      <th>Spending Score (1-100)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "      <td>31</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>196</td>\n",
       "      <td>Female</td>\n",
       "      <td>35</td>\n",
       "      <td>120</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>197</td>\n",
       "      <td>Female</td>\n",
       "      <td>45</td>\n",
       "      <td>126</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>198</td>\n",
       "      <td>Male</td>\n",
       "      <td>32</td>\n",
       "      <td>126</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>199</td>\n",
       "      <td>Male</td>\n",
       "      <td>32</td>\n",
       "      <td>137</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>200</td>\n",
       "      <td>Male</td>\n",
       "      <td>30</td>\n",
       "      <td>137</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     CustomerID   Genre  Age  Annual Income (k$)  Spending Score (1-100)\n",
       "0             1    Male   19                  15                      39\n",
       "1             2    Male   21                  15                      81\n",
       "2             3  Female   20                  16                       6\n",
       "3             4  Female   23                  16                      77\n",
       "4             5  Female   31                  17                      40\n",
       "..          ...     ...  ...                 ...                     ...\n",
       "195         196  Female   35                 120                      79\n",
       "196         197  Female   45                 126                      28\n",
       "197         198    Male   32                 126                      74\n",
       "198         199    Male   32                 137                      18\n",
       "199         200    Male   30                 137                      83\n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Independent Variables\n",
    "\n",
    "x = dataset.iloc[:, [3, 4]].values  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we are extracting only 3rd and 4th feature. It is because we need a 2d plot to visualize the model, and some features are not required, such as customer_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-2: Finding the optimal number of clusters using the elbow method\n",
    "In the second step, we will try to find the optimal number of clusters for our clustering problem. So, as discussed above, here we are going to use the elbow method for this purpose.\n",
    "\n",
    "As we know, the elbow method uses the WCSS concept to draw the plot by plotting WCSS values on the Y-axis and the number of clusters on the X-axis. So we are going to calculate the value for WCSS for different k values ranging from 1 to 10. Below is the code for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jvtay\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:882: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  f\"KMeans is known to have a memory leak on Windows \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#finding optimal number of clusters using the elbow method  \n",
    "from sklearn.cluster import KMeans  \n",
    "wcss_list= []  #Initializing the list for the values of WCSS  \n",
    "  \n",
    "#Using for loop for iterations from 1 to 10.  \n",
    "for i in range(1, 11):  \n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state= 42)  \n",
    "    kmeans.fit(x)  \n",
    "    wcss_list.append(kmeans.inertia_)  \n",
    "mtp.plot(range(1, 11), wcss_list)  \n",
    "mtp.title('The Elbow Method Graph')  \n",
    "mtp.xlabel('Number of clusters(k)')  \n",
    "mtp.ylabel('wcss_list')  \n",
    "mtp.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above code, we have used the KMeans class of sklearn. cluster library to form the clusters.\n",
    "\n",
    "Next, we have created the wcss_list variable to initialize an empty list, which is used to contain the value of wcss computed for different values of k ranging from 1 to 10.\n",
    "\n",
    "After that, we have initialized the for loop for the iteration on a different value of k ranging from 1 to 10; since for loop in Python, exclude the outbound limit, so it is taken as 11 to include 10th value.\n",
    "\n",
    "The rest part of the code is similar as we did in earlier topics, as we have fitted the model on a matrix of features and then plotted the graph between the number of clusters and WCSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step- 3: Training the K-means algorithm on the training dataset\n",
    "As we have got the number of clusters, so we can now train the model on the dataset.\n",
    "\n",
    "To train the model, we will use the same two lines of code as we have used in the above section, but here instead of using i, we will use 5, as we know there are 5 clusters that need to be formed. The code is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the K-means model on a dataset  \n",
    "kmeans = KMeans(n_clusters=5, init='k-means++', random_state= 42)  \n",
    "y_predict= kmeans.fit_predict(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3,\n",
       "       2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 0,\n",
       "       2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 4, 0, 4, 1, 4, 1, 4,\n",
       "       0, 4, 1, 4, 1, 4, 1, 4, 1, 4, 0, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4,\n",
       "       1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4,\n",
       "       1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4,\n",
       "       1, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://static.javatpoint.com/tutorial/machine-learning/images/k-means-clustering-algorithm-in-machine-learning18.png width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above image, we can now relate that the CustomerID 1 belongs to a cluster \n",
    "\n",
    "3(as index starts from 0, hence 2 will be considered as 3), and 2 belongs to cluster 4, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-4: Visualizing the Clusters\n",
    "The last step is to visualize the clusters. As we have 5 clusters for our model, so we will visualize each cluster one by one.\n",
    "\n",
    "To visualize the clusters will use scatter plot using mtp.scatter() function of matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visulaizing the clusters  \n",
    "mtp.scatter(x[y_predict == 0, 0], x[y_predict == 0, 1], s = 100, c = 'blue', label = 'Cluster 1') #for first cluster  \n",
    "mtp.scatter(x[y_predict == 1, 0], x[y_predict == 1, 1], s = 100, c = 'green', label = 'Cluster 2') #for second cluster  \n",
    "mtp.scatter(x[y_predict== 2, 0], x[y_predict == 2, 1], s = 100, c = 'red', label = 'Cluster 3') #for third cluster  \n",
    "mtp.scatter(x[y_predict == 3, 0], x[y_predict == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4') #for fourth cluster  \n",
    "mtp.scatter(x[y_predict == 4, 0], x[y_predict == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5') #for fifth cluster  \n",
    "mtp.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroid')   \n",
    "mtp.title('Clusters of customers')  \n",
    "mtp.xlabel('Annual Income (k$)')  \n",
    "mtp.ylabel('Spending Score (1-100)')  \n",
    "mtp.legend()  \n",
    "mtp.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above lines of code, we have written code for each clusters, ranging from 1 to 5. The first coordinate of the mtp.scatter, i.e., x[y_predict == 0, 0] containing the x value for the showing the matrix of features values, and the y_predict is ranging from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output image is clearly showing the five different clusters with different colors. The clusters are formed between two parameters of the dataset; Annual income of customer and Spending. We can change the colors and labels as per the requirement or choice. We can also observe some points from the above patterns, which are given below:\n",
    "\n",
    "- Cluster1 shows the customers with average salary and average spending so we can categorize these customers as\n",
    "- Cluster2 shows the customer has a high income but low spending, so we can categorize them as careful.\n",
    "- Cluster3 shows the low income and also low spending so they can be categorized as sensible.\n",
    "- Cluster4 shows the customers with low income with very high spending so they can be categorized as careless.\n",
    "- Cluster5 shows the customers with high income and high spending so they can be categorized as target, and these customers can be the most profitable customers for the mall owner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Algorithm\n",
    "The Perceptron algorithm is a **two-class (binary) classification** machine learning algorithm.\n",
    "\n",
    "It is a type of **neural network model**, perhaps the simplest type of neural network model.\n",
    "\n",
    "It consists of a single node or neuron that takes a row of data as input and predicts a class label. This is achieved by calculating the weighted sum of the inputs and a bias (set to 1). The weighted sum of the input of the model is called the activation.\n",
    "\n",
    "$Activation = Weights * Inputs + Bias$\n",
    "\n",
    "If the activation is above 0.0, the model will output 1.0; otherwise, it will output 0.0.\n",
    "\n",
    "- Predict 1: If Activation > 0.0\n",
    "- Predict 0: If Activation <= 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perceptron is a **linear classification algorithm**. This means that it learns a decision boundary that separates two classes using a line (called a **hyperplane**) in the feature space. As such, it is appropriate for those problems where the classes can be separated well by a line or linear model, referred to as **linearly separable**.\n",
    "\n",
    "The coefficients of the model are referred to as input weights and are trained using the **stochastic gradient descent** optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron With Scikit-Learn\n",
    "The Perceptron algorithm is available in the scikit-learn Python machine learning library via the Perceptron class.\n",
    "\n",
    "The class allows you to configure the learning rate (eta0), which defaults to 1.0.\n",
    "\n",
    "First, letâ€™s define a synthetic classification dataset.We will use the make_classification() function to create a dataset with 1,000 examples, each with 20 input variables.\n",
    "\n",
    "The example creates and summarizes the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# test classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=10, \n",
    "                           n_redundant=0, random_state=1)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "model = Perceptron(eta0=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation also allows you to configure the total number of training epochs (max_iter), which defaults to 1,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Perceptron(max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn implementation of the Perceptron algorithm also provides other configuration options that you may want to explore, such as early stopping and the use of a penalty loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.847 (0.052)\n"
     ]
    }
   ],
   "source": [
    "#complete example\n",
    "# evaluate a perceptron model on the dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import Perceptron\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, \n",
    "                           n_informative=10, n_redundant=0, \n",
    "                           random_state=1)\n",
    "# define model\n",
    "model = Perceptron(max_iter=1000, verbose=3)\n",
    "# define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# summarize result\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example evaluates the Perceptron algorithm on the synthetic dataset and reports the average accuracy across the three repeats of 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 1\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "\n",
    "# make a prediction with a perceptron model on the dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import Perceptron\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, \n",
    "                           n_features=10, \n",
    "                           n_informative=10, \n",
    "                           n_redundant=0, random_state=1)\n",
    "# define model\n",
    "model = Perceptron()\n",
    "# fit model\n",
    "model.fit(X, y)\n",
    "# define new data\n",
    "row = [0.12777556,-3.64400522,-2.23268854,\n",
    "       -1.82114386,1.75466361,0.1243966,\n",
    "       1.03397657,2.35822076,1.01001752,0.56768485]\n",
    "# make a prediction\n",
    "yhat = model.predict([row])\n",
    "# summarize prediction\n",
    "print('Predicted Class: %d' % yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Perceptron Hyperparameters\n",
    "The hyperparameters for the Perceptron algorithm must be configured for your specific dataset.\n",
    "\n",
    "Perhaps the most important hyperparameter is the learning rate.\n",
    "\n",
    "A large learning rate can cause the model to learn fast, but perhaps at the cost of lower skill. A smaller learning rate can result in a better-performing model but may take a long time to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define grid\n",
    "grid = dict()\n",
    "grid['eta0'] = [0.0001, 0.001, 0.01, 0.1, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.857\n",
      "Config: {'eta0': 0.0001}\n",
      ">0.857 with: {'eta0': 0.0001}\n",
      ">0.857 with: {'eta0': 0.001}\n",
      ">0.853 with: {'eta0': 0.01}\n",
      ">0.847 with: {'eta0': 0.1}\n",
      ">0.847 with: {'eta0': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# grid search learning rate for the perceptron\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import Perceptron\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=10, n_redundant=0, random_state=1)\n",
    "# define model\n",
    "model = Perceptron()\n",
    "# define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define grid\n",
    "grid = dict()\n",
    "grid['eta0'] = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "# define search\n",
    "search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "results = search.fit(X, y)\n",
    "# summarize\n",
    "print('Mean Accuracy: %.3f' % results.best_score_)\n",
    "print('Config: %s' % results.best_params_)\n",
    "# summarize all\n",
    "means = results.cv_results_['mean_test_score']\n",
    "params = results.cv_results_['params']\n",
    "for mean, param in zip(means, params):\n",
    "    print(\">%.3f with: %r\" % (mean, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important hyperparameter is how many epochs are used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define grid\n",
    "grid = dict()\n",
    "grid['max_iter'] = [1, 10, 100, 1000, 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "#We will use our well-performing learning rate of 0.0001 found in the previous search.\n",
    "\n",
    "\n",
    "model = Perceptron(eta0=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.857\n",
      "Config: {'max_iter': 10}\n",
      ">0.850 with: {'max_iter': 1}\n",
      ">0.857 with: {'max_iter': 10}\n",
      ">0.857 with: {'max_iter': 100}\n",
      ">0.857 with: {'max_iter': 1000}\n",
      ">0.857 with: {'max_iter': 10000}\n"
     ]
    }
   ],
   "source": [
    "# grid search total epochs for the perceptron\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import Perceptron\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=10, n_redundant=0, random_state=1)\n",
    "# define model\n",
    "model = Perceptron(eta0=0.0001)\n",
    "# define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define grid\n",
    "grid = dict()\n",
    "grid['max_iter'] = [1, 10, 100, 1000, 10000]\n",
    "# define search\n",
    "search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "results = search.fit(X, y)\n",
    "# summarize\n",
    "print('Mean Accuracy: %.3f' % results.best_score_)\n",
    "print('Config: %s' % results.best_params_)\n",
    "# summarize all\n",
    "means = results.cv_results_['mean_test_score']\n",
    "params = results.cv_results_['params']\n",
    "for mean, param in zip(means, params):\n",
    "    print(\">%.3f with: %r\" % (mean, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Tutorial Overview\n",
    "\n",
    "The steps we are going to cover in this tutorial are as follows:\n",
    "\n",
    "- Load Data.\n",
    "- Define Keras Model.\n",
    "- Compile Keras Model.\n",
    "- Fit Keras Model.\n",
    "- Evaluate Keras Model.\n",
    "- Tie It All Together.\n",
    "- Make Predictions\n",
    "\n",
    "This Keras tutorial has a few requirements:\n",
    "\n",
    "- You have Python 2 or 3 installed and configured.\n",
    "- You have SciPy (including NumPy) installed and configured.\n",
    "- You have Keras and a backend (Theano or TensorFlow) installed and configured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Data\n",
    "The first step is to define the functions and classes we intend to use in this tutorial.\n",
    "\n",
    "We will use the NumPy library to load our dataset and we will use two classes from the Keras library to define our model.\n",
    "\n",
    "The imports required are listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# first neural network with keras tutorial\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the file as a matrix of numbers using the NumPy function loadtxt().\n",
    "\n",
    "There are eight input variables and one output variable (the last column). We will be learning a model to map rows of input variables (X) to an output variable (y), which we often summarize as y = f(X).\n",
    "\n",
    "The variables can be summarized as follows:\n",
    "\n",
    "Input Variables (X):\n",
    "\n",
    "1. Number of times pregnant\n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "3. Diastolic blood pressure (mm Hg)\n",
    "4. Triceps skin fold thickness (mm)\n",
    "5. 2-Hour serum insulin (mu U/ml)\n",
    "6. Body mass index (weight in kg/(height in m)^2)\n",
    "7. Diabetes pedigree function\n",
    "8. Age (years)\n",
    "\n",
    "Output Variables (y):\n",
    "- Class variable (0 or 1)\n",
    "\n",
    "Once the CSV file is loaded into memory, we can split the columns of data into input and output variables.\n",
    "\n",
    "We can split the array into two arrays by selecting subsets of columns using the standard NumPy slice operator or â€œ:â€ We can select the first 8 columns from index 0 to index 7 via the slice 0:8. We can then select the output column (the 9th variable) via index 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "from numpy import loadtxt\n",
    "\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define Keras Model\n",
    "Models in Keras are defined as a sequence of layers.\n",
    "\n",
    "We create a Sequential model and add layers one at a time until we are happy with our network architecture.\n",
    "\n",
    "The first thing to get right is to ensure the input layer has the right number of input features. This can be specified when creating the first layer with the input_dim argument and setting it to 8 for the 8 input variables.\n",
    "\n",
    "How do we know the number of layers and their types?\n",
    "\n",
    "This is a very hard question. There are heuristics that we can use and often the best network structure is found through a process of trial and error experimentation (I explain more about this here). Generally, you need a network large enough to capture the structure of the problem.\n",
    "\n",
    "In this example, we will use a fully-connected network structure with three layers.\n",
    "\n",
    "Fully connected layers are defined using the Dense class. We can specify the number of neurons or nodes in the layer as the first argument, and specify the activation function using the activation argument.\n",
    "\n",
    "We will use the rectified linear unit activation function referred to as ReLU on the first two layers and the Sigmoid function in the output layer.\n",
    "\n",
    "It used to be the case that Sigmoid and Tanh activation functions were preferred for all layers. These days, better performance is achieved using the ReLU activation function. We use a sigmoid on the output layer to ensure our network output is between 0 and 1 and easy to map to either a probability of class 1 or snap to a hard classification of either class with a default threshold of 0.5.\n",
    "\n",
    "We can piece it all together by adding each layer:\n",
    "\n",
    "- The model expects rows of data with 8 variables (the input_dim=8 argument)\n",
    "- The first hidden layer has 12 nodes and uses the relu activation function.\n",
    "- The second hidden layer has 8 nodes and uses the relu activation function.\n",
    "- The output layer has one node and uses the sigmoid activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__, the most confusing thing here is that the shape of the input to the model is defined as an argument on the first hidden layer. This means that the line of code that adds the first Dense layer is doing 2 things, defining the input or visible layer and the first hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compile Keras Model\n",
    "Now that the model is defined, we can compile it.\n",
    "\n",
    "Compiling the model uses the efficient numerical libraries under the covers (the so-called backend) such as Theano or TensorFlow. The backend automatically chooses the best way to represent the network for training and making predictions to run on your hardware, such as CPU or GPU or even distributed.\n",
    "\n",
    "When compiling, we must specify some additional properties required when training the network. Remember training a network means finding the best set of weights to map inputs to outputs in our dataset.\n",
    "\n",
    "We must specify the loss function to use to evaluate a set of weights, the optimizer is used to search through different weights for the network and any optional metrics we would like to collect and report during training.\n",
    "\n",
    "In this case, we will use cross entropy as the loss argument. This loss is for a binary classification problems and is defined in Keras as â€œbinary_crossentropyâ€œ.\n",
    "\n",
    "We will define the optimizer as the efficient stochastic gradient descent algorithm â€œadamâ€œ. This is a popular version of gradient descent because it automatically tunes itself and gives good results in a wide range of problems. \n",
    "\n",
    "Finally, because it is a classification problem, we will collect and report the classification accuracy, defined via the metrics argument.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fit Keras Model\n",
    "We have defined our model and compiled it ready for efficient computation.\n",
    "\n",
    "Now it is time to execute the model on some data.\n",
    "\n",
    "We can train or fit our model on our loaded data by calling the fit() function on the model.\n",
    "\n",
    "Training occurs over epochs and each epoch is split into batches.\n",
    "\n",
    "- Epoch: One pass through all of the rows in the training dataset.\n",
    "- Batch: One or more samples considered by the model within an epoch before weights are updated.\n",
    "\n",
    "One epoch is comprised of one or more batches, based on the chosen batch size and the model is fit for many epochs. \n",
    "\n",
    "The training process will run for a fixed number of iterations through the dataset called epochs, that we must specify using the epochs argument. We must also set the number of dataset rows that are considered before the model weights are updated within each epoch, called the batch size and set using the batch_size argument.\n",
    "\n",
    "For this problem, we will run for a small number of epochs (150) and use a relatively small batch size of 10.\n",
    "\n",
    "These configurations can be chosen experimentally by trial and error. We want to train the model enough so that it learns a good (or good enough) mapping of rows of input data to the output classification. The model will always have some error, but the amount of error will level out after some point for a given model configuration. This is called model convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(X, y, epochs=150, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the work happens on your CPU or GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if your GPU is working\n",
    "Requirements:\n",
    "\n",
    "- tensorflow - 2.1.0\n",
    "- tensorflow-base - 2.1.0\n",
    "- tensorflow-estimator - 2.1.0\n",
    "- tensorflow-gpu - 2.1.0\n",
    "- keras - 2.3.1 \n",
    "- keras - 2.3.1\n",
    "- cudnn - 7.6.5\n",
    "- cudatoolkit - 10.1.243\n",
    "- python = 3.6.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15989299439290721698\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3057333044\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17961919326302624228\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate Keras Model\n",
    "We have trained our neural network on the entire dataset and we can evaluate the performance of the network on the same dataset.\n",
    "\n",
    "This will only give us an idea of how well we have modeled the dataset (e.g. train accuracy), but no idea of how well the algorithm might perform on new data. We have done this for simplicity, but ideally, you could separate your data into train and test datasets for training and evaluation of your model.\n",
    "\n",
    "You can evaluate your model on your training dataset using the evaluate() function on your model and pass it the same input and output used to train the model.\n",
    "\n",
    "This will generate a prediction for each input and output pair and collect scores, including the average loss and any metrics you have configured, such as accuracy.\n",
    "\n",
    "The evaluate() function will return a list with two values. The first will be the loss of the model on the dataset and the second will be the accuracy of the model on the dataset. We are only interested in reporting the accuracy, so we will ignore the loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Tie It All Together\n",
    "You have just seen how you can easily create your first neural network model in Keras.\n",
    "\n",
    "Letâ€™s tie it all together into a complete code example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "768/768 [==============================] - 1s 929us/step - loss: 22.5737 - accuracy: 0.6510\n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s 518us/step - loss: 4.9057 - accuracy: 0.5951\n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s 455us/step - loss: 1.5005 - accuracy: 0.5742\n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.9397 - accuracy: 0.6068\n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s 478us/step - loss: 0.8309 - accuracy: 0.6159\n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s 453us/step - loss: 0.7823 - accuracy: 0.5951\n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s 408us/step - loss: 0.7474 - accuracy: 0.6120\n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s 470us/step - loss: 0.7325 - accuracy: 0.6198\n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s 464us/step - loss: 0.7059 - accuracy: 0.6107\n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s 425us/step - loss: 0.7131 - accuracy: 0.5964\n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.7041 - accuracy: 0.6094\n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.6906 - accuracy: 0.6055\n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s 476us/step - loss: 0.6872 - accuracy: 0.6328\n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s 480us/step - loss: 0.6826 - accuracy: 0.6081\n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s 456us/step - loss: 0.6573 - accuracy: 0.6419\n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s 463us/step - loss: 0.6647 - accuracy: 0.6510\n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s 481us/step - loss: 0.6558 - accuracy: 0.6510\n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s 486us/step - loss: 0.6486 - accuracy: 0.6771\n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.6626 - accuracy: 0.6354\n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.6527 - accuracy: 0.6641\n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.6521 - accuracy: 0.66930s - loss: 0.6266 - accuracy\n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.6482 - accuracy: 0.6576\n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s 461us/step - loss: 0.6346 - accuracy: 0.6667\n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s 579us/step - loss: 0.6213 - accuracy: 0.6602\n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 1s 677us/step - loss: 0.6280 - accuracy: 0.6628\n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 1s 751us/step - loss: 0.6126 - accuracy: 0.6680\n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 1s 720us/step - loss: 0.6088 - accuracy: 0.6823\n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 1s 751us/step - loss: 0.6259 - accuracy: 0.6576\n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 1s 780us/step - loss: 0.6231 - accuracy: 0.6693\n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 1s 739us/step - loss: 0.6097 - accuracy: 0.6784\n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 1s 786us/step - loss: 0.6026 - accuracy: 0.6719\n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 1s 726us/step - loss: 0.6091 - accuracy: 0.6667\n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 1s 852us/step - loss: 0.6006 - accuracy: 0.6562\n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 1s 813us/step - loss: 0.5991 - accuracy: 0.6680\n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 1s 742us/step - loss: 0.5790 - accuracy: 0.6784\n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 1s 796us/step - loss: 0.5861 - accuracy: 0.6810\n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 1s 751us/step - loss: 0.5834 - accuracy: 0.6940\n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 1s 756us/step - loss: 0.5957 - accuracy: 0.6719\n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 1s 772us/step - loss: 0.5730 - accuracy: 0.6810\n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 1s 766us/step - loss: 0.5983 - accuracy: 0.6810\n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 1s 710us/step - loss: 0.5821 - accuracy: 0.6888\n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 1s 780us/step - loss: 0.5810 - accuracy: 0.7005\n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 1s 765us/step - loss: 0.5807 - accuracy: 0.6862\n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 1s 734us/step - loss: 0.6000 - accuracy: 0.6797\n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 1s 831us/step - loss: 0.5774 - accuracy: 0.6823\n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 1s 758us/step - loss: 0.5762 - accuracy: 0.6901\n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 1s 697us/step - loss: 0.5891 - accuracy: 0.6849\n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 1s 762us/step - loss: 0.6157 - accuracy: 0.6836\n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 1s 710us/step - loss: 0.5848 - accuracy: 0.6771\n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 1s 704us/step - loss: 0.5721 - accuracy: 0.6940\n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 1s 730us/step - loss: 0.5672 - accuracy: 0.6992\n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 1s 822us/step - loss: 0.5789 - accuracy: 0.6940\n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 1s 812us/step - loss: 0.5707 - accuracy: 0.6849\n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 1s 789us/step - loss: 0.5662 - accuracy: 0.6992\n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 1s 727us/step - loss: 0.5860 - accuracy: 0.6901\n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 1s 735us/step - loss: 0.5788 - accuracy: 0.6979\n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 1s 718us/step - loss: 0.5612 - accuracy: 0.7018\n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 1s 695us/step - loss: 0.5693 - accuracy: 0.6940\n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 1s 769us/step - loss: 0.5762 - accuracy: 0.6979\n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 1s 751us/step - loss: 0.5684 - accuracy: 0.7070\n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 1s 804us/step - loss: 0.5648 - accuracy: 0.7005\n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 1s 757us/step - loss: 0.5635 - accuracy: 0.7018\n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 1s 778us/step - loss: 0.5631 - accuracy: 0.6979\n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 1s 780us/step - loss: 0.5841 - accuracy: 0.6979\n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 1s 739us/step - loss: 0.5509 - accuracy: 0.7018\n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 1s 701us/step - loss: 0.5577 - accuracy: 0.7070\n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 1s 774us/step - loss: 0.5734 - accuracy: 0.7018\n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 1s 684us/step - loss: 0.5800 - accuracy: 0.6901\n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 1s 751us/step - loss: 0.5542 - accuracy: 0.7005\n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 1s 687us/step - loss: 0.5649 - accuracy: 0.6940\n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 1s 697us/step - loss: 0.5595 - accuracy: 0.6992\n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 1s 710us/step - loss: 0.5505 - accuracy: 0.6979\n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 1s 680us/step - loss: 0.5495 - accuracy: 0.7135\n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 1s 737us/step - loss: 0.5517 - accuracy: 0.7096\n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 1s 714us/step - loss: 0.5738 - accuracy: 0.6940\n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 1s 750us/step - loss: 0.5705 - accuracy: 0.7031\n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 1s 724us/step - loss: 0.5812 - accuracy: 0.6784\n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 1s 803us/step - loss: 0.5625 - accuracy: 0.7044\n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 1s 676us/step - loss: 0.5562 - accuracy: 0.6927\n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 1s 785us/step - loss: 0.5699 - accuracy: 0.6927\n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 1s 785us/step - loss: 0.5636 - accuracy: 0.6953\n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 1s 719us/step - loss: 0.5592 - accuracy: 0.6953\n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 1s 759us/step - loss: 0.5630 - accuracy: 0.7083\n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 1s 744us/step - loss: 0.5817 - accuracy: 0.6693\n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 1s 709us/step - loss: 0.5554 - accuracy: 0.7057\n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 1s 693us/step - loss: 0.5588 - accuracy: 0.6992\n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 1s 703us/step - loss: 0.5525 - accuracy: 0.7161\n",
      "Epoch 88/150\n",
      "768/768 [==============================] - 1s 760us/step - loss: 0.5613 - accuracy: 0.7005\n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 1s 712us/step - loss: 0.5535 - accuracy: 0.7083\n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 1s 743us/step - loss: 0.5644 - accuracy: 0.7044\n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 1s 754us/step - loss: 0.5355 - accuracy: 0.7292\n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 1s 687us/step - loss: 0.5440 - accuracy: 0.7057\n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 1s 755us/step - loss: 0.5406 - accuracy: 0.7057\n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 1s 790us/step - loss: 0.5456 - accuracy: 0.7057\n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 1s 709us/step - loss: 0.5427 - accuracy: 0.7083\n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 1s 736us/step - loss: 0.5431 - accuracy: 0.7031\n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 1s 725us/step - loss: 0.5495 - accuracy: 0.7070\n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 1s 761us/step - loss: 0.5661 - accuracy: 0.6901\n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 1s 714us/step - loss: 0.5423 - accuracy: 0.7044\n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 1s 752us/step - loss: 0.5545 - accuracy: 0.7148\n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 1s 746us/step - loss: 0.5508 - accuracy: 0.6953\n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 1s 663us/step - loss: 0.5584 - accuracy: 0.7005\n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 1s 727us/step - loss: 0.5475 - accuracy: 0.7174\n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 1s 769us/step - loss: 0.5559 - accuracy: 0.7044\n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 1s 769us/step - loss: 0.5543 - accuracy: 0.7057\n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 1s 764us/step - loss: 0.5365 - accuracy: 0.7109\n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 1s 763us/step - loss: 0.5375 - accuracy: 0.7122\n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 1s 731us/step - loss: 0.5482 - accuracy: 0.7057\n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 1s 719us/step - loss: 0.5486 - accuracy: 0.7044\n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 1s 739us/step - loss: 0.5496 - accuracy: 0.7161\n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 1s 728us/step - loss: 0.5365 - accuracy: 0.7005\n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 1s 699us/step - loss: 0.5591 - accuracy: 0.7044\n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 1s 734us/step - loss: 0.5408 - accuracy: 0.7083\n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 1s 686us/step - loss: 0.5405 - accuracy: 0.7070\n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 1s 767us/step - loss: 0.5424 - accuracy: 0.7174\n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 1s 780us/step - loss: 0.5382 - accuracy: 0.7188\n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 1s 672us/step - loss: 0.5477 - accuracy: 0.7096\n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 1s 781us/step - loss: 0.5394 - accuracy: 0.7240\n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 1s 758us/step - loss: 0.5548 - accuracy: 0.7096\n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 1s 812us/step - loss: 0.5324 - accuracy: 0.7161\n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 1s 737us/step - loss: 0.5421 - accuracy: 0.7174\n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 1s 749us/step - loss: 0.5455 - accuracy: 0.7122\n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 1s 781us/step - loss: 0.5498 - accuracy: 0.7201\n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 1s 712us/step - loss: 0.5528 - accuracy: 0.7122\n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 1s 718us/step - loss: 0.5396 - accuracy: 0.7227\n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 1s 805us/step - loss: 0.5372 - accuracy: 0.7161\n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 1s 751us/step - loss: 0.5503 - accuracy: 0.7148\n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 1s 762us/step - loss: 0.5493 - accuracy: 0.7201\n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 1s 754us/step - loss: 0.5353 - accuracy: 0.7083\n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 1s 768us/step - loss: 0.5370 - accuracy: 0.7161\n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 1s 710us/step - loss: 0.5391 - accuracy: 0.7148\n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 1s 725us/step - loss: 0.5490 - accuracy: 0.7188\n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 1s 718us/step - loss: 0.5337 - accuracy: 0.7292\n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 1s 745us/step - loss: 0.5300 - accuracy: 0.7031\n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 1s 703us/step - loss: 0.5633 - accuracy: 0.7096\n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 1s 735us/step - loss: 0.5475 - accuracy: 0.7227\n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 1s 783us/step - loss: 0.5477 - accuracy: 0.7044\n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 1s 722us/step - loss: 0.5433 - accuracy: 0.7083\n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 1s 726us/step - loss: 0.5425 - accuracy: 0.7174\n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 1s 742us/step - loss: 0.5474 - accuracy: 0.7122\n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 1s 765us/step - loss: 0.5370 - accuracy: 0.7161\n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 1s 791us/step - loss: 0.5382 - accuracy: 0.7214\n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 1s 740us/step - loss: 0.5436 - accuracy: 0.7240\n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 1s 742us/step - loss: 0.5282 - accuracy: 0.7135\n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 1s 731us/step - loss: 0.5202 - accuracy: 0.7253\n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 1s 743us/step - loss: 0.5351 - accuracy: 0.7148\n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 1s 790us/step - loss: 0.5418 - accuracy: 0.7161\n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 1s 816us/step - loss: 0.5291 - accuracy: 0.7240\n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 1s 747us/step - loss: 0.5307 - accuracy: 0.7240\n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 1s 778us/step - loss: 0.5319 - accuracy: 0.7292\n",
      "768/768 [==============================] - 0s 109us/step\n",
      "Accuracy: 73.05\n"
     ]
    }
   ],
   "source": [
    "# first neural network with keras tutorial\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# load the dataset\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X, y, epochs=150, batch_size=10)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Make Predictions\n",
    "The number one question I get asked is:\n",
    "\n",
    "After I train my model, how can I use it to make predictions on new data?\n",
    "\n",
    "Making predictions is as easy as calling the predict() function on the model. We are using a sigmoid activation function on the output layer, so the predictions will be a probability in the range between 0 and 1. We can easily convert them into a crisp binary prediction for this classification task by rounding them.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make probability predictions with the model\n",
    "predictions = model.predict(X)\n",
    "# round predictions \n",
    "rounded = [round(x[0]) for x in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternately, we can call the predict_classes() function on the model to predict crisp classes directly, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions with the model\n",
    "predictions = model.predict_classes(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0] => 1 (expected 1)\n",
      "[1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0] => 0 (expected 0)\n",
      "[8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 0.672, 32.0] => 1 (expected 1)\n",
      "[1.0, 89.0, 66.0, 23.0, 94.0, 28.1, 0.167, 21.0] => 0 (expected 0)\n",
      "[0.0, 137.0, 40.0, 35.0, 168.0, 43.1, 2.288, 33.0] => 1 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "# Complete Example\n",
    "\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# load the dataset\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X, y, epochs=150, batch_size=10, verbose=3)\n",
    "# make class predictions with the model\n",
    "predictions = model.predict_classes(X)\n",
    "# summarize the first 5 cases\n",
    "for i in range(5):\n",
    "\tprint('%s => %d (expected %d)' % (X[i].tolist(), predictions[i], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, you learned the six key steps in using Keras to create a neural network or deep learning model, step-by-step including:\n",
    "\n",
    "1. How to load data.\n",
    "2. How to define a neural network in Keras.\n",
    "3. How to compile a Keras model using the efficient numerical backend.\n",
    "4. How to train a model on data.\n",
    "5. How to evaluate a model on data.\n",
    "6. How to make predictions with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras Tutorial Extensions\n",
    "\n",
    "This section provides some extensions that you might want to explore.\n",
    "\n",
    "- Tune the Model. Change the configuration of the model or training process and see if you can improve the performance of the model, e.g. achieve better than 76% accuracy.\n",
    "- Save the Model. Update the tutorial to save the model to file, then load it later and use it to make predictions (see this tutorial).\n",
    "- Summarize the Model. Update the tutorial to summarize the model and create a plot of model layers (see this tutorial).\n",
    "- Separate Train and Test Datasets. Split the loaded dataset into a train and test set (split based on rows) and use one set to train the model and the other set to estimate the performance of the model on new data.\n",
    "- Plot Learning Curves. The fit() function returns a history object that summarizes the loss and accuracy at the end of each epoch. Create line plots of this data, called learning curves (see this tutorial).\n",
    "- Learn a New Dataset. Update the tutorial to use a different tabular dataset, perhaps from the UCI Machine Learning Repository.\n",
    "- Use Functional API. Update the tutorial to use the Keras Functional API for defining the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Tune Batch Size and Number of Epochs\n",
    "The batch size in iterative gradient descent is the number of patterns shown to the network before the weights are updated. It is also an optimization in the training of the network, defining how many patterns to read at a time and keep in memory.\n",
    "\n",
    "The number of epochs is the number of times that the entire training dataset is shown to the network during training. Some networks are sensitive to the batch size, such as LSTM recurrent neural networks and Convolutional Neural Networks.\n",
    "\n",
    "Here we will evaluate a suite of different mini batch sizes from 10 to 100 in steps of 20.\n",
    "\n",
    "The full code listing is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Epoch 2/100\n",
      "Epoch 3/100\n",
      "Epoch 4/100\n",
      "Epoch 5/100\n",
      "Epoch 6/100\n",
      "Epoch 7/100\n",
      "Epoch 8/100\n",
      "Epoch 9/100\n",
      "Epoch 10/100\n",
      "Epoch 11/100\n",
      "Epoch 12/100\n",
      "Epoch 13/100\n",
      "Epoch 14/100\n",
      "Epoch 15/100\n",
      "Epoch 16/100\n",
      "Epoch 17/100\n",
      "Epoch 18/100\n",
      "Epoch 19/100\n",
      "Epoch 20/100\n",
      "Epoch 21/100\n",
      "Epoch 22/100\n",
      "Epoch 23/100\n",
      "Epoch 24/100\n",
      "Epoch 25/100\n",
      "Epoch 26/100\n",
      "Epoch 27/100\n",
      "Epoch 28/100\n",
      "Epoch 29/100\n",
      "Epoch 30/100\n",
      "Epoch 31/100\n",
      "Epoch 32/100\n",
      "Epoch 33/100\n",
      "Epoch 34/100\n",
      "Epoch 35/100\n",
      "Epoch 36/100\n",
      "Epoch 37/100\n",
      "Epoch 38/100\n",
      "Epoch 39/100\n",
      "Epoch 40/100\n",
      "Epoch 41/100\n",
      "Epoch 42/100\n",
      "Epoch 43/100\n",
      "Epoch 44/100\n",
      "Epoch 45/100\n",
      "Epoch 46/100\n",
      "Epoch 47/100\n",
      "Epoch 48/100\n",
      "Epoch 49/100\n",
      "Epoch 50/100\n",
      "Epoch 51/100\n",
      "Epoch 52/100\n",
      "Epoch 53/100\n",
      "Epoch 54/100\n",
      "Epoch 55/100\n",
      "Epoch 56/100\n",
      "Epoch 57/100\n",
      "Epoch 58/100\n",
      "Epoch 59/100\n",
      "Epoch 60/100\n",
      "Epoch 61/100\n",
      "Epoch 62/100\n",
      "Epoch 63/100\n",
      "Epoch 64/100\n",
      "Epoch 65/100\n",
      "Epoch 66/100\n",
      "Epoch 67/100\n",
      "Epoch 68/100\n",
      "Epoch 69/100\n",
      "Epoch 70/100\n",
      "Epoch 71/100\n",
      "Epoch 72/100\n",
      "Epoch 73/100\n",
      "Epoch 74/100\n",
      "Epoch 75/100\n",
      "Epoch 76/100\n",
      "Epoch 77/100\n",
      "Epoch 78/100\n",
      "Epoch 79/100\n",
      "Epoch 80/100\n",
      "Epoch 81/100\n",
      "Epoch 82/100\n",
      "Epoch 83/100\n",
      "Epoch 84/100\n",
      "Epoch 85/100\n",
      "Epoch 86/100\n",
      "Epoch 87/100\n",
      "Epoch 88/100\n",
      "Epoch 89/100\n",
      "Epoch 90/100\n",
      "Epoch 91/100\n",
      "Epoch 92/100\n",
      "Epoch 93/100\n",
      "Epoch 94/100\n",
      "Epoch 95/100\n",
      "Epoch 96/100\n",
      "Epoch 97/100\n",
      "Epoch 98/100\n",
      "Epoch 99/100\n",
      "Epoch 100/100\n",
      "Best: 0.700521 using {'batch_size': 10, 'epochs': 100}\n",
      "0.609375 (0.044993) with: {'batch_size': 10, 'epochs': 10}\n",
      "0.647135 (0.050058) with: {'batch_size': 10, 'epochs': 50}\n",
      "0.700521 (0.003683) with: {'batch_size': 10, 'epochs': 100}\n",
      "0.630208 (0.013279) with: {'batch_size': 20, 'epochs': 10}\n",
      "0.645833 (0.028940) with: {'batch_size': 20, 'epochs': 50}\n",
      "0.675781 (0.020915) with: {'batch_size': 20, 'epochs': 100}\n",
      "0.576823 (0.030145) with: {'batch_size': 40, 'epochs': 10}\n",
      "0.645833 (0.011201) with: {'batch_size': 40, 'epochs': 50}\n",
      "0.674479 (0.012890) with: {'batch_size': 40, 'epochs': 100}\n",
      "0.542969 (0.055335) with: {'batch_size': 60, 'epochs': 10}\n",
      "0.657552 (0.022402) with: {'batch_size': 60, 'epochs': 50}\n",
      "0.669271 (0.032734) with: {'batch_size': 60, 'epochs': 100}\n",
      "0.501302 (0.117028) with: {'batch_size': 80, 'epochs': 10}\n",
      "0.664062 (0.024080) with: {'batch_size': 80, 'epochs': 50}\n",
      "0.636719 (0.039192) with: {'batch_size': 80, 'epochs': 100}\n",
      "0.463542 (0.054345) with: {'batch_size': 100, 'epochs': 10}\n",
      "0.621094 (0.054501) with: {'batch_size': 100, 'epochs': 50}\n",
      "0.639323 (0.051658) with: {'batch_size': 100, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(12, input_dim=8, activation='relu'))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=3)\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7005208333333334"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 10, 'epochs': 100}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Tune the Training Optimization Algorithm\n",
    "Keras offers a suite of different state-of-the-art optimization algorithms.\n",
    "\n",
    "In this example, we tune the optimization algorithm used to train the network, each with default parameters.\n",
    "\n",
    "This is an odd example, because often you will choose one approach a priori and instead focus on tuning its parameters on your problem (e.g. see the next example).\n",
    "\n",
    "Here we will evaluate the suite of optimization algorithms supported by the Keras API.\n",
    "\n",
    "The full code listing is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 1s - loss: 29.5959 - accuracy: 0.3724\n",
      "Epoch 2/100\n",
      " - 1s - loss: 8.1678 - accuracy: 0.5664\n",
      "Epoch 3/100\n",
      " - 0s - loss: 4.3889 - accuracy: 0.6042\n",
      "Epoch 4/100\n",
      " - 0s - loss: 3.1386 - accuracy: 0.6172\n",
      "Epoch 5/100\n",
      " - 0s - loss: 2.4003 - accuracy: 0.6159\n",
      "Epoch 6/100\n",
      " - 0s - loss: 1.7941 - accuracy: 0.6276\n",
      "Epoch 7/100\n",
      " - 0s - loss: 1.4026 - accuracy: 0.6198\n",
      "Epoch 8/100\n",
      " - 0s - loss: 1.1636 - accuracy: 0.6211\n",
      "Epoch 9/100\n",
      " - 0s - loss: 1.0638 - accuracy: 0.6263\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.9774 - accuracy: 0.6406\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.9422 - accuracy: 0.6432\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.8833 - accuracy: 0.6562\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.8560 - accuracy: 0.6536\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.8460 - accuracy: 0.6589\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.8302 - accuracy: 0.6615\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.7990 - accuracy: 0.6680\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.7919 - accuracy: 0.6797\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.7436 - accuracy: 0.6784\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.7379 - accuracy: 0.6732\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.7128 - accuracy: 0.6901\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.7049 - accuracy: 0.6862\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.7094 - accuracy: 0.6862\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.6907 - accuracy: 0.6810\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.6624 - accuracy: 0.6940\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.6795 - accuracy: 0.6914\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.6861 - accuracy: 0.6810\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.6554 - accuracy: 0.6927\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.6527 - accuracy: 0.7044\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.6347 - accuracy: 0.6992\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.6403 - accuracy: 0.6992\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.6456 - accuracy: 0.6927\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.6265 - accuracy: 0.7174\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.6202 - accuracy: 0.7253\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.6131 - accuracy: 0.7214\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.6375 - accuracy: 0.6797\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.6219 - accuracy: 0.7031\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.6136 - accuracy: 0.7070\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.6051 - accuracy: 0.7227\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.6140 - accuracy: 0.7005\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.6077 - accuracy: 0.7161\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.6077 - accuracy: 0.7122\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.5956 - accuracy: 0.7109\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.5893 - accuracy: 0.7214\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.6042 - accuracy: 0.7214\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.6115 - accuracy: 0.6979\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.5917 - accuracy: 0.7227\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.5904 - accuracy: 0.7266\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.5924 - accuracy: 0.7227\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.5845 - accuracy: 0.7357\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.5915 - accuracy: 0.7148\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.6032 - accuracy: 0.7161\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.6069 - accuracy: 0.6979\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.5846 - accuracy: 0.7188\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.5745 - accuracy: 0.7344\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.5794 - accuracy: 0.7266\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.5778 - accuracy: 0.7253\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.5744 - accuracy: 0.7240\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.5847 - accuracy: 0.7292\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.5825 - accuracy: 0.7227\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.6014 - accuracy: 0.7201\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.5826 - accuracy: 0.7279\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.5852 - accuracy: 0.7188\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.5858 - accuracy: 0.7279\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.5828 - accuracy: 0.7148\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.5938 - accuracy: 0.7188\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.5756 - accuracy: 0.7266\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.5610 - accuracy: 0.7174\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.6205 - accuracy: 0.7109\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.5783 - accuracy: 0.7201\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.5733 - accuracy: 0.7201\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.5601 - accuracy: 0.7370\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.5933 - accuracy: 0.7240\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.5803 - accuracy: 0.7161\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.5563 - accuracy: 0.7331\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.5610 - accuracy: 0.7266\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.5600 - accuracy: 0.7331\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.5611 - accuracy: 0.7174\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.5816 - accuracy: 0.7135\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.5694 - accuracy: 0.7409\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.5753 - accuracy: 0.7161\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.5606 - accuracy: 0.7240\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.5470 - accuracy: 0.7318\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.5497 - accuracy: 0.7448\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.5509 - accuracy: 0.7370\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.5512 - accuracy: 0.7409\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.5534 - accuracy: 0.7448\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.5471 - accuracy: 0.7383\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.5711 - accuracy: 0.7253\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.5539 - accuracy: 0.7474\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.5465 - accuracy: 0.7279\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.5571 - accuracy: 0.7383\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.5590 - accuracy: 0.7526\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.5576 - accuracy: 0.7357\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.5665 - accuracy: 0.7292\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.5497 - accuracy: 0.7305\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.5492 - accuracy: 0.7370\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.5468 - accuracy: 0.7448\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.5575 - accuracy: 0.7370\n",
      "Epoch 99/100\n",
      " - 1s - loss: 0.5374 - accuracy: 0.7500\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.5341 - accuracy: 0.7474\n",
      "Best: 0.699219 using {'optimizer': 'Adam'}\n",
      "0.645833 (0.024360) with: {'optimizer': 'SGD'}\n",
      "0.660156 (0.038273) with: {'optimizer': 'RMSprop'}\n",
      "0.652344 (0.030425) with: {'optimizer': 'Adagrad'}\n",
      "0.692708 (0.010253) with: {'optimizer': 'Adadelta'}\n",
      "0.699219 (0.038670) with: {'optimizer': 'Adam'}\n",
      "0.699219 (0.015947) with: {'optimizer': 'Adamax'}\n",
      "0.677083 (0.008027) with: {'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='adam'):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(12, input_dim=8, activation='relu'))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\treturn model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=2)\n",
    "# define the grid search parameters\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Tune Learning Rate and Momentum\n",
    "It is common to pre-select an optimization algorithm to train your network and tune its parameters.\n",
    "\n",
    "By far the most common optimization algorithm is plain old Stochastic Gradient Descent (SGD) because it is so well understood. In this example, we will look at optimizing the SGD learning rate and momentum parameters.\n",
    "\n",
    "Learning rate controls how much to update the weight at the end of each batch and the momentum controls how much to let the previous update influence the current weight update.\n",
    "\n",
    "We will try a suite of small standard learning rates and a momentum values from 0.2 to 0.8 in steps of 0.2, as well as 0.9 (because it can be a popular value in practice).\n",
    "\n",
    "Generally, it is a good idea to also include the number of epochs in an optimization like this as there is a dependency between the amount of learning per batch (learning rate), the number of updates per epoch (batch size) and the number of epochs.\n",
    "\n",
    "The full code listing is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "768/768 [==============================] - 1s 773us/step - loss: 3.6562 - accuracy: 0.5742\n",
      "Epoch 2/100\n",
      "768/768 [==============================] - 0s 455us/step - loss: 1.7209 - accuracy: 0.5951\n",
      "Epoch 3/100\n",
      "768/768 [==============================] - 0s 495us/step - loss: 1.1129 - accuracy: 0.6146\n",
      "Epoch 4/100\n",
      "768/768 [==============================] - 0s 499us/step - loss: 0.9985 - accuracy: 0.6107\n",
      "Epoch 5/100\n",
      "768/768 [==============================] - 0s 477us/step - loss: 0.9442 - accuracy: 0.6029\n",
      "Epoch 6/100\n",
      "768/768 [==============================] - 0s 566us/step - loss: 0.8014 - accuracy: 0.6393\n",
      "Epoch 7/100\n",
      "768/768 [==============================] - 0s 470us/step - loss: 0.7428 - accuracy: 0.6263\n",
      "Epoch 8/100\n",
      "768/768 [==============================] - 0s 483us/step - loss: 0.7991 - accuracy: 0.6159\n",
      "Epoch 9/100\n",
      "768/768 [==============================] - 0s 421us/step - loss: 0.7612 - accuracy: 0.6328\n",
      "Epoch 10/100\n",
      "768/768 [==============================] - 0s 544us/step - loss: 0.6812 - accuracy: 0.6562\n",
      "Epoch 11/100\n",
      "768/768 [==============================] - 0s 473us/step - loss: 0.7423 - accuracy: 0.6276\n",
      "Epoch 12/100\n",
      "768/768 [==============================] - 0s 347us/step - loss: 0.6671 - accuracy: 0.6576\n",
      "Epoch 13/100\n",
      "768/768 [==============================] - 0s 413us/step - loss: 0.6709 - accuracy: 0.6484\n",
      "Epoch 14/100\n",
      "768/768 [==============================] - 0s 551us/step - loss: 0.6484 - accuracy: 0.6758\n",
      "Epoch 15/100\n",
      "768/768 [==============================] - 0s 525us/step - loss: 0.6425 - accuracy: 0.6523\n",
      "Epoch 16/100\n",
      "768/768 [==============================] - 0s 461us/step - loss: 0.6443 - accuracy: 0.6654\n",
      "Epoch 17/100\n",
      "768/768 [==============================] - 0s 486us/step - loss: 0.6313 - accuracy: 0.6641\n",
      "Epoch 18/100\n",
      "768/768 [==============================] - 0s 418us/step - loss: 0.6366 - accuracy: 0.6732\n",
      "Epoch 19/100\n",
      "768/768 [==============================] - 0s 403us/step - loss: 0.6233 - accuracy: 0.6693\n",
      "Epoch 20/100\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.6297 - accuracy: 0.6849\n",
      "Epoch 21/100\n",
      "768/768 [==============================] - 0s 418us/step - loss: 0.6253 - accuracy: 0.6536\n",
      "Epoch 22/100\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.6295 - accuracy: 0.6719\n",
      "Epoch 23/100\n",
      "768/768 [==============================] - 0s 388us/step - loss: 0.6284 - accuracy: 0.6576\n",
      "Epoch 24/100\n",
      "768/768 [==============================] - 0s 369us/step - loss: 0.6258 - accuracy: 0.6693\n",
      "Epoch 25/100\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.6140 - accuracy: 0.6914\n",
      "Epoch 26/100\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.6068 - accuracy: 0.6875\n",
      "Epoch 27/100\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.6182 - accuracy: 0.6745\n",
      "Epoch 28/100\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.6129 - accuracy: 0.6953\n",
      "Epoch 29/100\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.6291 - accuracy: 0.6719\n",
      "Epoch 30/100\n",
      "768/768 [==============================] - 0s 405us/step - loss: 0.6086 - accuracy: 0.6836\n",
      "Epoch 31/100\n",
      "768/768 [==============================] - 0s 340us/step - loss: 0.6063 - accuracy: 0.7083\n",
      "Epoch 32/100\n",
      "768/768 [==============================] - 0s 403us/step - loss: 0.6045 - accuracy: 0.6888\n",
      "Epoch 33/100\n",
      "768/768 [==============================] - 0s 449us/step - loss: 0.6130 - accuracy: 0.6927\n",
      "Epoch 34/100\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.6102 - accuracy: 0.6862\n",
      "Epoch 35/100\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.6097 - accuracy: 0.6745\n",
      "Epoch 36/100\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.6049 - accuracy: 0.6914\n",
      "Epoch 37/100\n",
      "768/768 [==============================] - 0s 434us/step - loss: 0.6099 - accuracy: 0.6862\n",
      "Epoch 38/100\n",
      "768/768 [==============================] - 0s 462us/step - loss: 0.6004 - accuracy: 0.6953\n",
      "Epoch 39/100\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.5950 - accuracy: 0.7005\n",
      "Epoch 40/100\n",
      "768/768 [==============================] - 0s 347us/step - loss: 0.5973 - accuracy: 0.6927\n",
      "Epoch 41/100\n",
      "768/768 [==============================] - 0s 379us/step - loss: 0.6012 - accuracy: 0.6836\n",
      "Epoch 42/100\n",
      "768/768 [==============================] - 0s 358us/step - loss: 0.5990 - accuracy: 0.6901\n",
      "Epoch 43/100\n",
      "768/768 [==============================] - 0s 408us/step - loss: 0.5906 - accuracy: 0.6953\n",
      "Epoch 44/100\n",
      "768/768 [==============================] - 0s 409us/step - loss: 0.5964 - accuracy: 0.6862\n",
      "Epoch 45/100\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.5935 - accuracy: 0.6927\n",
      "Epoch 46/100\n",
      "768/768 [==============================] - 0s 327us/step - loss: 0.5965 - accuracy: 0.7174\n",
      "Epoch 47/100\n",
      "768/768 [==============================] - 0s 378us/step - loss: 0.5867 - accuracy: 0.6901\n",
      "Epoch 48/100\n",
      "768/768 [==============================] - 0s 392us/step - loss: 0.5932 - accuracy: 0.6940\n",
      "Epoch 49/100\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.5852 - accuracy: 0.6979\n",
      "Epoch 50/100\n",
      "768/768 [==============================] - 0s 406us/step - loss: 0.6018 - accuracy: 0.6940\n",
      "Epoch 51/100\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.5915 - accuracy: 0.7031\n",
      "Epoch 52/100\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.5871 - accuracy: 0.6927\n",
      "Epoch 53/100\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.5954 - accuracy: 0.7148\n",
      "Epoch 54/100\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.5883 - accuracy: 0.7005\n",
      "Epoch 55/100\n",
      "768/768 [==============================] - 0s 434us/step - loss: 0.5936 - accuracy: 0.6966\n",
      "Epoch 56/100\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.5879 - accuracy: 0.7005\n",
      "Epoch 57/100\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.5871 - accuracy: 0.7109\n",
      "Epoch 58/100\n",
      "768/768 [==============================] - 0s 392us/step - loss: 0.5890 - accuracy: 0.7109\n",
      "Epoch 59/100\n",
      "768/768 [==============================] - 0s 423us/step - loss: 0.5885 - accuracy: 0.6979\n",
      "Epoch 60/100\n",
      "768/768 [==============================] - 0s 466us/step - loss: 0.5794 - accuracy: 0.7135\n",
      "Epoch 61/100\n",
      "768/768 [==============================] - 0s 436us/step - loss: 0.5836 - accuracy: 0.7227\n",
      "Epoch 62/100\n",
      "768/768 [==============================] - 0s 388us/step - loss: 0.5788 - accuracy: 0.7018\n",
      "Epoch 63/100\n",
      "768/768 [==============================] - 0s 474us/step - loss: 0.5831 - accuracy: 0.7070\n",
      "Epoch 64/100\n",
      "768/768 [==============================] - 0s 436us/step - loss: 0.5858 - accuracy: 0.6953\n",
      "Epoch 65/100\n",
      "768/768 [==============================] - 0s 497us/step - loss: 0.5841 - accuracy: 0.7018\n",
      "Epoch 66/100\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.5841 - accuracy: 0.7135\n",
      "Epoch 67/100\n",
      "768/768 [==============================] - 0s 418us/step - loss: 0.5799 - accuracy: 0.7044\n",
      "Epoch 68/100\n",
      "768/768 [==============================] - 0s 456us/step - loss: 0.5798 - accuracy: 0.6966\n",
      "Epoch 69/100\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.5800 - accuracy: 0.7057\n",
      "Epoch 70/100\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.5843 - accuracy: 0.6888\n",
      "Epoch 71/100\n",
      "768/768 [==============================] - 0s 388us/step - loss: 0.5836 - accuracy: 0.7188\n",
      "Epoch 72/100\n",
      "768/768 [==============================] - 0s 382us/step - loss: 0.5837 - accuracy: 0.7188\n",
      "Epoch 73/100\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.5783 - accuracy: 0.6940\n",
      "Epoch 74/100\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.5800 - accuracy: 0.7122\n",
      "Epoch 75/100\n",
      "768/768 [==============================] - 0s 456us/step - loss: 0.5807 - accuracy: 0.7005\n",
      "Epoch 76/100\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.5765 - accuracy: 0.7188\n",
      "Epoch 77/100\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.5826 - accuracy: 0.7083\n",
      "Epoch 78/100\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.5738 - accuracy: 0.7122\n",
      "Epoch 79/100\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.5775 - accuracy: 0.7057\n",
      "Epoch 80/100\n",
      "768/768 [==============================] - 0s 434us/step - loss: 0.5833 - accuracy: 0.7070\n",
      "Epoch 81/100\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.5785 - accuracy: 0.7044\n",
      "Epoch 82/100\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.5742 - accuracy: 0.7083\n",
      "Epoch 83/100\n",
      "768/768 [==============================] - 0s 418us/step - loss: 0.5771 - accuracy: 0.7044\n",
      "Epoch 84/100\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.5772 - accuracy: 0.6979\n",
      "Epoch 85/100\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.5760 - accuracy: 0.7122\n",
      "Epoch 86/100\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.5808 - accuracy: 0.7018\n",
      "Epoch 87/100\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.5804 - accuracy: 0.7070\n",
      "Epoch 88/100\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.5764 - accuracy: 0.6875\n",
      "Epoch 89/100\n",
      "768/768 [==============================] - 0s 404us/step - loss: 0.5772 - accuracy: 0.7057\n",
      "Epoch 90/100\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.5823 - accuracy: 0.6888\n",
      "Epoch 91/100\n",
      "768/768 [==============================] - 0s 449us/step - loss: 0.5711 - accuracy: 0.7135\n",
      "Epoch 92/100\n",
      "768/768 [==============================] - 0s 453us/step - loss: 0.5766 - accuracy: 0.7096\n",
      "Epoch 93/100\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.5728 - accuracy: 0.7031\n",
      "Epoch 94/100\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.5746 - accuracy: 0.7031\n",
      "Epoch 95/100\n",
      "768/768 [==============================] - 0s 419us/step - loss: 0.5803 - accuracy: 0.6966\n",
      "Epoch 96/100\n",
      "768/768 [==============================] - 0s 445us/step - loss: 0.5747 - accuracy: 0.7174\n",
      "Epoch 97/100\n",
      "768/768 [==============================] - 0s 436us/step - loss: 0.5740 - accuracy: 0.7018\n",
      "Epoch 98/100\n",
      "768/768 [==============================] - 0s 418us/step - loss: 0.5766 - accuracy: 0.7031\n",
      "Epoch 99/100\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.5762 - accuracy: 0.7005\n",
      "Epoch 100/100\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.5754 - accuracy: 0.7018\n",
      "Best: 0.697917 using {'learn_rate': 0.001, 'momentum': 0.4}\n",
      "0.673177 (0.020752) with: {'learn_rate': 0.001, 'momentum': 0.0}\n",
      "0.666667 (0.012075) with: {'learn_rate': 0.001, 'momentum': 0.2}\n",
      "0.697917 (0.029463) with: {'learn_rate': 0.001, 'momentum': 0.4}\n",
      "0.670573 (0.021236) with: {'learn_rate': 0.001, 'momentum': 0.6}\n",
      "0.656250 (0.005524) with: {'learn_rate': 0.001, 'momentum': 0.8}\n",
      "0.648438 (0.028348) with: {'learn_rate': 0.001, 'momentum': 0.9}\n",
      "0.665365 (0.051263) with: {'learn_rate': 0.01, 'momentum': 0.0}\n",
      "0.654948 (0.035132) with: {'learn_rate': 0.01, 'momentum': 0.2}\n",
      "0.649740 (0.026557) with: {'learn_rate': 0.01, 'momentum': 0.4}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.01, 'momentum': 0.6}\n",
      "0.648438 (0.028348) with: {'learn_rate': 0.01, 'momentum': 0.8}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.01, 'momentum': 0.9}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.0}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.2}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.4}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.6}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.8}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.1, 'momentum': 0.9}\n",
      "0.652344 (0.026107) with: {'learn_rate': 0.2, 'momentum': 0.0}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.2}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.4}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.6}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.8}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.2, 'momentum': 0.9}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.0}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.2}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.4}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.6}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.8}\n",
      "0.651042 (0.024774) with: {'learn_rate': 0.3, 'momentum': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the learning rate and momentum\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import SGD\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(12, input_dim=8, activation='relu'))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\toptimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\treturn model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=1)\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Tune Network Weight Initialization\n",
    "Neural network weight initialization used to be simple: use small random values.\n",
    "\n",
    "Now there is a suite of different techniques to choose from. Keras provides a laundry list.\n",
    "\n",
    "In this example, we will look at tuning the selection of network weight initialization by evaluating all of the available techniques.\n",
    "\n",
    "We will use the same weight initialization method on each layer. Ideally, it may be better to use different weight initialization schemes according to the activation function used on each layer. In the example below we use rectifier for the hidden layer. We use sigmoid for the output layer because the predictions are binary.\n",
    "\n",
    "The full code listing is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.718750 using {'init_mode': 'glorot_normal'}\n",
      "0.709635 (0.015733) with: {'init_mode': 'uniform'}\n",
      "0.704427 (0.006639) with: {'init_mode': 'lecun_uniform'}\n",
      "0.713542 (0.011201) with: {'init_mode': 'normal'}\n",
      "0.651042 (0.024774) with: {'init_mode': 'zero'}\n",
      "0.718750 (0.000000) with: {'init_mode': 'glorot_normal'}\n",
      "0.688802 (0.016367) with: {'init_mode': 'glorot_uniform'}\n",
      "0.688802 (0.020256) with: {'init_mode': 'he_normal'}\n",
      "0.687500 (0.005524) with: {'init_mode': 'he_uniform'}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the weight initialization\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(init_mode='uniform'):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(12, input_dim=8, kernel_initializer=init_mode, activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_grid = dict(init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Tune the Neuron Activation Function\n",
    "The activation function controls the non-linearity of individual neurons and when to fire.\n",
    "\n",
    "Generally, the rectifier activation function is the most popular, but it used to be the sigmoid and the tanh functions and these functions may still be more suitable for different problems.\n",
    "\n",
    "In this example, we will evaluate the suite of different activation functions available in Keras. We will only use these functions in the hidden layer, as we require a sigmoid activation function in the output for the binary classification problem.\n",
    "\n",
    "Generally, it is a good idea to prepare data to the range of the different transfer functions, which we will not do in this case.\n",
    "\n",
    "The full code listing is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.723958 using {'activation': 'softplus'}\n",
      "0.657552 (0.003683) with: {'activation': 'softmax'}\n",
      "0.723958 (0.025976) with: {'activation': 'softplus'}\n",
      "0.684896 (0.003683) with: {'activation': 'softsign'}\n",
      "0.717448 (0.024360) with: {'activation': 'relu'}\n",
      "0.697917 (0.030647) with: {'activation': 'tanh'}\n",
      "0.695312 (0.013902) with: {'activation': 'sigmoid'}\n",
      "0.691406 (0.016877) with: {'activation': 'hard_sigmoid'}\n",
      "0.704427 (0.001841) with: {'activation': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the activation function\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(activation='relu'):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation=activation))\n",
    "\tmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Tune Dropout Regularization\n",
    "In this example, we will look at tuning the dropout rate for regularization in an effort to limit overfitting and improve the modelâ€™s ability to generalize.\n",
    "\n",
    "To get good results, dropout is best combined with a weight constraint such as the max norm constraint.\n",
    "\n",
    "This involves fitting both the dropout percentage and the weight constraint. We will try dropout percentages between 0.0 and 0.9 (1.0 does not make sense) and maxnorm weight constraint values between 0 and 5.\n",
    "\n",
    "The full code listing is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn to grid search the dropout rate\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(dropout_rate=0.0, weight_constraint=0):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(weight_constraint)))\n",
    "\tmodel.add(Dropout(dropout_rate))\n",
    "\tmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "weight_constraint = [1, 2, 3, 4, 5]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Tune the Number of Neurons in the Hidden Layer\n",
    "The number of neurons in a layer is an important parameter to tune. Generally the number of neurons in a layer controls the representational capacity of the network, at least at that point in the topology.\n",
    "\n",
    "Also, generally, a large enough single layer network can approximate any other neural network, at least in theory.\n",
    "\n",
    "In this example, we will look at tuning the number of neurons in a single hidden layer. We will try values from 1 to 30 in steps of 5.\n",
    "\n",
    "A larger network requires more training and at least the batch size and number of epochs should ideally be optimized with the number of neurons.\n",
    "\n",
    "The full code listing is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn to grid search the number of neurons\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(neurons=1):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(neurons, input_dim=8, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(4)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
    "param_grid = dict(neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips for Hyperparameter Optimization\n",
    "This section lists some handy tips to consider when tuning hyperparameters of your neural network.\n",
    "\n",
    "- **k-fold Cross Validation**. You can see that the results from the examples in this post show some variance. A default cross-validation of 3 was used, but perhaps k=5 or k=10 would be more stable. Carefully choose your cross validation configuration to ensure your results are stable.\n",
    "- **Review the Whole Grid**. Do not just focus on the best result, review the whole grid of results and look for trends to support configuration decisions.\n",
    "- **Parallelize**. Use all your cores if you can, neural networks are slow to train and we often want to try a lot of different parameters. Consider spinning up a lot of AWS instances.\n",
    "- **Use a Sample of Your Dataset**. Because networks are slow to train, try training them on a smaller sample of your training dataset, just to get an idea of general directions of parameters rather than optimal configurations.\n",
    "- **Start with Coarse Grids**. Start with coarse-grained grids and zoom into finer grained grids once you can narrow the scope.\n",
    "- **Do not Transfer Results**. Results are generally problem specific. Try to avoid favorite configurations on each new problem that you see. It is unlikely that optimal results you discover on one problem will transfer to your next project. Instead look for broader trends like number of layers or relationships between parameters.\n",
    "- **Reproducibility is a Problem**. Although we set the seed for the random number generator in NumPy, the results are not 100% reproducible. There is more to reproducibility when grid searching wrapped Keras models than is presented in this post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
